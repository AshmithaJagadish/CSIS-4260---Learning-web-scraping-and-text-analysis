{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b4d380-a1dd-43c8-92b9-fa63baeec9cc",
   "metadata": {},
   "source": [
    "# Introduction Summary: Different Scraping Approach\n",
    "\n",
    "This code demonstrates how to use **BeautifulSoup, lxml.html, MechanicalSoup** (in combination with the `requests` library) to scrape articles from BBC News. The script performs the following steps:\n",
    "\n",
    "1. **Fetch the Homepage**:  \n",
    "   It sends an HTTP request to the BBC News homepage with appropriate headers to mimic a real browser.\n",
    "\n",
    "2. **Parse the HTML Content**:  \n",
    "   BeautifulSoup parses the HTML content, allowing the script to search for specific elements using CSS selectors. In this case, it extracts all `<a>` tags containing the string `/news/` in their `href` attribute.\n",
    "\n",
    "3. **Construct and Filter Article URLs**:  \n",
    "   The code converts relative URLs to absolute URLs, removes duplicates, and then limits the set to a defined number of links (e.g., the first 150).\n",
    "\n",
    "4. **Scrape Individual Articles**:  \n",
    "   For each article URL, the script sends a new HTTP request, parses the article page, and extracts:\n",
    "   - The article title (from the first `<h1>` element)\n",
    "   - The article content (by concatenating text from all `<p>` elements)\n",
    "\n",
    "5. **Save the Data**:  \n",
    "   The extracted title and content are saved into a CSV file for further analysis or use.\n",
    "\n",
    "6. **Performance Measurement**:  \n",
    "   The code tracks the time taken to complete the scraping process and reports the total number of articles scraped.\n",
    "\n",
    "**Key Benefits**:  \n",
    "- **Simplicity**: Easy to implement with minimal setup.\n",
    "- **Flexibility**: Allows custom extraction logic using CSS selectors.\n",
    "- **Speed**: Effective for small-to-medium scale scraping tasks.\n",
    "\n",
    "**Limitations**:  \n",
    "- The code depends on the structure of the BBC homepage and may require adjustments if the HTML layout changes.\n",
    "- It relies on manual URL filtering and does not handle pagination, which might limit the total number of articles scraped.\n",
    "\n",
    "This approach is ideal for quick prototypes or smaller projects where ease of use and rapid development are prioritized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926862af-73e7-4707-87dc-2ae13d74f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import requests\n",
    "import lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "import mechanicalsoup\n",
    "\n",
    "# Set logging level for urllib3 to suppress DEBUG messages.\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4533f0-f87e-42dd-9bf2-3fe1be68ebfe",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c139d2f-cd70-4dac-83df-05f21ee2455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed: 81 articles saved in 6.05 seconds using BeautifulSoup.\n"
     ]
    }
   ],
   "source": [
    "# Define the BBC News homepage URL and set up request headers to mimic a browser.\n",
    "bbc_base_url = 'https://www.bbc.com/news'\n",
    "request_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Start timing the script.\n",
    "start_time = time.time()\n",
    "\n",
    "# Fetch the homepage content.\n",
    "homepage_resp = requests.get(bbc_base_url, headers=request_headers)\n",
    "homepage_soup = BeautifulSoup(homepage_resp.text, 'html.parser')\n",
    "\n",
    "# Extract article links from the homepage.\n",
    "article_links = []\n",
    "for tag in homepage_soup.find_all('a', href=True):\n",
    "    link_href = tag['href']\n",
    "    if link_href.startswith('/news/') and '/news/' in link_href:\n",
    "        article_links.append('https://www.bbc.com' + link_href)\n",
    "\n",
    "# Retrieve and parse details for each article (limiting to the first 500 links).\n",
    "articles_info = []\n",
    "for url in article_links[:500]:\n",
    "    try:\n",
    "        art_resp = requests.get(url, headers=request_headers)\n",
    "        art_soup = BeautifulSoup(art_resp.text, 'html.parser')\n",
    "\n",
    "        # Get the article headline.\n",
    "        headline = art_soup.find('h1').get_text() if art_soup.find('h1') else 'No title'\n",
    "        # Combine all paragraph texts to form the article content.\n",
    "        paragraphs = art_soup.find_all('p')\n",
    "        article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "        articles_info.append({\n",
    "            'title': headline,\n",
    "            'url': url,\n",
    "            'content': article_text\n",
    "        })\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Save the scraped data into a CSV file.\n",
    "csv_filename = 'bbc_news_articles_bs.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['title', 'url', 'content'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(articles_info)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed: {len(articles_info)} articles saved in {end_time - start_time:.2f} seconds using BeautifulSoup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29cd88e-b1ac-496e-ae9a-2ff0d1f77522",
   "metadata": {},
   "source": [
    "## Lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5a176c-f44a-4bd7-a73a-973374b6dded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Found 53 unique article links.\n",
      "Scraping completed: 53 articles saved in 56.05 seconds.\n",
      "File saved at: C:\\Users\\300407353\\bbc_articles_lxml.csv\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# BBC News homepage URL and headers to mimic a browser.\n",
    "BBC_URL = \"https://www.bbc.com/news\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Fetch and parse the homepage.\n",
    "response = requests.get(BBC_URL, headers=HEADERS)\n",
    "tree = lxml.html.fromstring(response.content)\n",
    "\n",
    "# Extract article links using XPath.\n",
    "raw_links = tree.xpath('//a[contains(@href, \"/news/\")]/@href')\n",
    "# Build full URLs and remove duplicates.\n",
    "unique_links = list({f\"https://www.bbc.com{link}\" for link in raw_links if link.startswith(\"/news/\")})\n",
    "print(f\"DEBUG: Found {len(unique_links)} unique article links.\")\n",
    "\n",
    "# Limit to a subset (adjust as needed): get the first 150 links.\n",
    "unique_links = unique_links[:150]\n",
    "\n",
    "articles = []\n",
    "for url in unique_links:\n",
    "    try:\n",
    "        art_resp = requests.get(url, headers=HEADERS)\n",
    "        art_tree = lxml.html.fromstring(art_resp.content)\n",
    "        # Extract the title from the first <h1> element.\n",
    "        title_list = art_tree.xpath('//h1//text()')\n",
    "        title = title_list[0].strip() if title_list else \"No title\"\n",
    "        # Extract all paragraph texts.\n",
    "        paragraphs = art_tree.xpath('//p//text()')\n",
    "        content = \" \".join(p.strip() for p in paragraphs if p.strip())\n",
    "        # Append title, link, and content.\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"link\": url,\n",
    "            \"content\": content\n",
    "        })\n",
    "        time.sleep(1)  # Pause briefly to be polite.\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping article: {e}\")\n",
    "\n",
    "# Specify a path where you have write permission. For example, save to the home directory.\n",
    "output_path = os.path.join(os.path.expanduser(\"~\"), \"bbc_articles_lxml.csv\")\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"link\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(articles)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed: {len(articles)} articles saved in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"File saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfea626-f61a-47eb-9cef-40a5c52a0418",
   "metadata": {},
   "source": [
    "## Mechanical Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25de2c71-993c-41ed-aa85-9a6441c073b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Found 53 unique article links.\n",
      "Scraping completed: 53 articles saved in 56.87 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create a browser instance with a user agent.\n",
    "browser = mechanicalsoup.StatefulBrowser(\n",
    "    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "               \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "               \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# BBC News homepage URL.\n",
    "BBC_URL = \"https://www.bbc.com/news\"\n",
    "\n",
    "# Open the homepage.\n",
    "browser.open(BBC_URL)\n",
    "soup = browser.get_current_page()\n",
    "\n",
    "# Extract article links using a CSS selector.\n",
    "article_links = [a.get(\"href\") for a in soup.select('a[href*=\"/news/\"]')]\n",
    "# Construct full URLs and remove duplicates.\n",
    "unique_links = list({f\"https://www.bbc.com{link}\" for link in article_links if link.startswith(\"/news/\")})\n",
    "print(f\"DEBUG: Found {len(unique_links)} unique article links.\")\n",
    "\n",
    "# Limit to a subset (adjust as needed).\n",
    "unique_links = unique_links[:150]\n",
    "\n",
    "articles = []\n",
    "for url in unique_links:\n",
    "    try:\n",
    "        # Open each article page.\n",
    "        browser.open(url)\n",
    "        page_soup = browser.get_current_page()\n",
    "        # Extract the title from the first <h1> element.\n",
    "        title_tag = page_soup.find(\"h1\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"No title\"\n",
    "        # Extract article content from all paragraph elements.\n",
    "        paragraphs = page_soup.find_all(\"p\")\n",
    "        content = \" \".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "        # Append title, link, and content.\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"link\": url,\n",
    "            \"content\": content\n",
    "        })\n",
    "        time.sleep(1)  # Pause briefly to be polite.\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping an article: {e}\")\n",
    "\n",
    "# Save the scraped articles to a CSV file (including title, link, and content).\n",
    "csv_filename = \"bbc_article_mechanicalsoup.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"link\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(articles)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed: {len(articles)} articles saved in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d6792-461a-4724-bc2b-6839ac3947b1",
   "metadata": {},
   "source": [
    "# Comparison of Scraping Methods for BBC News\n",
    "\n",
    "Below is a comparison table summarizing the performance and characteristics of three scraping methods—**BeautifulSoup**, **lxml.html**, and **MechanicalSoup**—based on our tests (targeting 150 articles, but each method scraped fewer):\n",
    "\n",
    "| **Feature**                     | **BeautifulSoup**                                      | **lxml.html**                                                          | **MechanicalSoup**                                                   |\n",
    "|---------------------------------|--------------------------------------------------------|------------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **Articles Scraped** (Goal: 150)| 82                                                     | 54                                                                     | 53                                                                   |\n",
    "| **Time Taken (seconds)**        | 26.91                                                  | 64.36                                                                  | 58.15                                                                |\n",
    "| **Ease of Setup**               | Straightforward; minimal dependencies                  | Simple for XPath users; requires knowledge of lxml and XPath           | Easy to install; simulates a browser session (StatefulBrowser)         |\n",
    "| **Speed**                       | Fast for smaller tasks                                 | Slower under test conditions; efficient with well-formed HTML          | Moderate; slight overhead from browser-like session                  |\n",
    "| **Scalability**                 | Good for moderate projects; no built-in concurrency      | Good for moderate projects; no built-in concurrency                    | Good for moderate projects; no built-in concurrency                    |\n",
    "| **Code Complexity**             | Minimal; manual link collection & data parsing         | Requires understanding of XPath; code remains concise                   | Similar to BeautifulSoup but includes browser emulation approach       |\n",
    "| **Built-in Features**           | None; requires manual or third-party enhancements       | Pure parser; advanced features require custom logic                     | Provides session management via StatefulBrowser but limited concurrency|\n",
    "| **Reached 150 Articles?**       | No (87 articles)                                       | No (53 articles)                                                       | No (55 articles)                                                       |\n",
    "| **Limitations Observed**        | Limited links from BBC homepage                        | Fewer links found; HTML structure may reduce results                    | Limited by available links; similar browser-like restrictions           |\n",
    "| **Best Use Cases**              | Quick prototyping & small-to-medium scrapes             | When fine-grained XPath control is needed and HTML is well-structured     | Projects that benefit from session management and browser simulation    |\n",
    "| **Overall Observations**        | Fastest and returned the most articles in this test     | Slower and returned fewer articles; powerful for structured parsing       | Moderately fast; convenient for simulating browser actions              |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **All methods** fell short of the 150-article goal due to the BBC homepage’s limited unique links.\n",
    "- **BeautifulSoup** scraped the most articles (87) in the shortest time (26.91 seconds).\n",
    "- **lxml.html** allowed precise XPath-based parsing but returned fewer articles (53) in 64.36 seconds.\n",
    "- **MechanicalSoup** found 55 articles in 58.15 seconds, offering a browser-like session approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de627c0-f1df-4008-b204-8ff961d3d5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
